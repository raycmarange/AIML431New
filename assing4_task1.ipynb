{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCfnqiU75ZSucbTtU2RLJ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raycmarange/AIML431New/blob/main/assing4_task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt9XqYm3hvzi",
        "outputId": "500f6d26-1aaf-49f3-b343-10495c612d0a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for file at: SMSSpamCollection.txt\n",
            "Successfully loaded with tab delimiter\n",
            "Dataset loaded: 5572 samples\n",
            "Label distribution:\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n",
            "Label mapping result:\n",
            "label\n",
            "0    4825\n",
            "1     747\n",
            "Name: count, dtype: int64\n",
            "Missing values: label    0\n",
            "text     0\n",
            "dtype: int64\n",
            "Final dataset size: 5572\n",
            "Class distribution: {0: np.int64(4825), 1: np.int64(747)}\n",
            "Training samples: 4458\n",
            "Testing samples: 1114\n",
            "Test set class distribution: {0: 965, 1: 149}\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  97%|█████████▋| 272/279 [1:28:02<02:14, 19.19s/it, Loss=0.0151]"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import AdamW  # Import from torch instead of transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class BERTSpamClassifier:\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=128, batch_size=16):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.train_dataloader = None\n",
        "        self.test_dataloader = None\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        \"\"\"Load and preprocess the SMS spam dataset\"\"\"\n",
        "        print(f\"Looking for file at: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, delimiter='\\t', header=None, names=['label', 'text'])\n",
        "            print(\"Successfully loaded with tab delimiter\")\n",
        "        except:\n",
        "            df = pd.read_csv(file_path, encoding='latin-1', header=None, names=['label', 'text'])\n",
        "            print(\"Successfully loaded with latin-1 encoding\")\n",
        "\n",
        "        print(f\"Dataset loaded: {len(df)} samples\")\n",
        "        print(\"Label distribution:\")\n",
        "        print(df['label'].value_counts())\n",
        "\n",
        "        # Map labels to numerical values\n",
        "        df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "        print(\"Label mapping result:\")\n",
        "        print(df['label'].value_counts())\n",
        "\n",
        "        # Check for missing values\n",
        "        print(f\"Missing values: {df.isnull().sum()}\")\n",
        "        df = df.dropna()\n",
        "        print(f\"Final dataset size: {len(df)}\")\n",
        "\n",
        "        self.texts = df['text'].values\n",
        "        self.labels = df['label'].values\n",
        "        self.class_distribution = dict(df['label'].value_counts())\n",
        "        print(f\"Class distribution: {self.class_distribution}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_datasets(self, test_size=0.2):\n",
        "        \"\"\"Prepare train/test datasets\"\"\"\n",
        "        dataset = SpamDataset(self.texts, self.labels, self.tokenizer, self.max_length)\n",
        "\n",
        "        # Calculate split sizes\n",
        "        test_size = int(len(dataset) * test_size)\n",
        "        train_size = len(dataset) - test_size\n",
        "\n",
        "        # Split dataset\n",
        "        self.train_dataset, self.test_dataset = random_split(\n",
        "            dataset, [train_size, test_size]\n",
        "        )\n",
        "\n",
        "        print(f\"Training samples: {len(self.train_dataset)}\")\n",
        "        print(f\"Testing samples: {len(self.test_dataset)}\")\n",
        "\n",
        "        # Create data loaders\n",
        "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        # Print test set distribution\n",
        "        test_labels = [self.test_dataset[i]['labels'].item() for i in range(len(self.test_dataset))]\n",
        "        test_distribution = {0: test_labels.count(0), 1: test_labels.count(1)}\n",
        "        print(f\"Test set class distribution: {test_distribution}\")\n",
        "\n",
        "        return self.train_dataloader, self.test_dataloader\n",
        "\n",
        "    def train(self, epochs=3, learning_rate=2e-5):\n",
        "        \"\"\"Train the BERT model\"\"\"\n",
        "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            progress_bar = tqdm(self.train_dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "            for step, batch in enumerate(progress_bar):\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update progress bar\n",
        "                if step % 10 == 0:\n",
        "                    progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            avg_loss = total_loss / len(self.train_dataloader)\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the model on test set\"\"\"\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.test_dataloader, desc='Evaluating'):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                predictions.extend(preds.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        weighted_f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "        print(f\"Test set size: {len(true_labels)}\")\n",
        "        print(f\"Unique labels in test set: {np.unique(true_labels)}\")\n",
        "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
        "        print(f\"\\nClassification Report:\\n{classification_report(true_labels, predictions, target_names=['ham', 'spam'])}\")\n",
        "\n",
        "        return accuracy, weighted_f1, cm, predictions, true_labels\n",
        "\n",
        "    def create_tsne_visualization(self, n_samples=1000, random_state=42):\n",
        "        \"\"\"Create t-SNE visualization of hidden states\"\"\"\n",
        "        print(\"Creating t-SNE visualization...\")\n",
        "\n",
        "        # Use a subset if dataset is too large\n",
        "        if len(self.test_dataset) > n_samples:\n",
        "            indices = np.random.choice(len(self.test_dataset), n_samples, replace=False)\n",
        "            subset_dataset = torch.utils.data.Subset(self.test_dataset, indices)\n",
        "            dataloader = DataLoader(subset_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            print(f\"Using {n_samples} samples for t-SNE (random subset)\")\n",
        "        else:\n",
        "            dataloader = self.test_dataloader\n",
        "            print(f\"Fitting TSNE with {len(self.test_dataset)} samples...\")\n",
        "\n",
        "        # Get hidden states\n",
        "        self.model.eval()\n",
        "        hidden_states = []\n",
        "        labels_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc='Extracting hidden states'):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                batch_labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "\n",
        "                # Use the last hidden state and average over sequence length\n",
        "                last_hidden_state = outputs.hidden_states[-1]\n",
        "                averaged_hidden = last_hidden_state.mean(dim=1)\n",
        "\n",
        "                hidden_states.append(averaged_hidden.cpu().numpy())\n",
        "                labels_list.append(batch_labels.cpu().numpy())\n",
        "\n",
        "        hidden_states = np.vstack(hidden_states)\n",
        "        labels = np.hstack(labels_list)\n",
        "\n",
        "        # Apply t-SNE with error handling\n",
        "        try:\n",
        "            # Set environment variable to avoid the CPU core detection issue\n",
        "            import os\n",
        "            os.environ['LOKY_MAX_CPU_COUNT'] = '1'\n",
        "\n",
        "            tsne = TSNE(n_components=2, random_state=random_state, verbose=1, n_jobs=1)\n",
        "            hidden_2d = tsne.fit_transform(hidden_states)\n",
        "\n",
        "            # Create visualization\n",
        "            plt.figure(figsize=(12, 10))\n",
        "            scatter = plt.scatter(hidden_2d[:, 0], hidden_2d[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
        "            plt.colorbar(scatter, label='Label (0=ham, 1=spam)')\n",
        "            plt.title('t-SNE Visualization of BERT Hidden States')\n",
        "            plt.xlabel('t-SNE Component 1')\n",
        "            plt.ylabel('t-SNE Component 2')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Add legend\n",
        "            unique_labels = np.unique(labels)\n",
        "            for label in unique_labels:\n",
        "                plt.scatter([], [], c=['blue', 'orange'][label], label=f\"{'ham' if label == 0 else 'spam'} ({np.sum(labels == label)})\")\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            return hidden_2d, labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in t-SNE: {e}\")\n",
        "            print(\"Creating alternative visualization using PCA...\")\n",
        "            return self.create_pca_visualization(hidden_states, labels)\n",
        "\n",
        "    def create_pca_visualization(self, hidden_states, labels):\n",
        "        \"\"\"Fallback visualization using PCA\"\"\"\n",
        "        from sklearn.decomposition import PCA\n",
        "\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        hidden_2d = pca.fit_transform(hidden_states)\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        scatter = plt.scatter(hidden_2d[:, 0], hidden_2d[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
        "        plt.colorbar(scatter, label='Label (0=ham, 1=spam)')\n",
        "        plt.title('PCA Visualization of BERT Hidden States (Fallback)')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        unique_labels = np.unique(labels)\n",
        "        for label in unique_labels:\n",
        "            plt.scatter([], [], c=['blue', 'orange'][label], label=f\"{'ham' if label == 0 else 'spam'} ({np.sum(labels == label)})\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return hidden_2d, labels\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the trained model\"\"\"\n",
        "        torch.save(self.model.state_dict(), filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load a trained model\"\"\"\n",
        "        self.model.load_state_dict(torch.load(filepath, map_location=self.device))\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "\n",
        "def main():\n",
        "    # Initialize classifier\n",
        "    classifier = BERTSpamClassifier(batch_size=16)\n",
        "\n",
        "    # Load data\n",
        "    file_path = \"SMSSpamCollection.txt\"\n",
        "    classifier.load_data(file_path)\n",
        "\n",
        "    # Prepare datasets\n",
        "    classifier.prepare_datasets(test_size=0.2)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    classifier.train(epochs=3)\n",
        "\n",
        "    # Evaluate model\n",
        "    accuracy, f1, cm, predictions, true_labels = classifier.evaluate()\n",
        "\n",
        "    # Create t-SNE visualization\n",
        "    classifier.create_tsne_visualization(n_samples=1000)\n",
        "\n",
        "    # Save model\n",
        "    classifier.save_model(\"bert_spam_classifier_fixed.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}